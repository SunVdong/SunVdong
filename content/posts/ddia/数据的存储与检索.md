+++
title = "数据的存储与检索"
date = 2023-04-03T16:25:57+08:00
draft = false
author = 'vdong'
categories = ['技术']
tags = ['数据密集', 'SSTable', 'LSM-tree']

+++

## 最简单的数据库

```shell
#!/bin/bash
db_set () {
	echo "$1,$2" >> database
}
db_get () {
	grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

使用方法

```shell
source db.sh
db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'
db_get 123456
```

该数据库，写入性能很好，但是读取性能非常糟糕，查找开销 `O(n)` 。

为了高效的查找数据可特有的键，我需要一个数据结构：索引（index）。即额外保存一些元数据作为路标，帮助找到我们想要的数据。索引是主数据的附加数据，在解决提升查询性能这一问题的同时，引入了新的问题，即写入数据时，要额外维护索引，也就产生了额外开销。这是一个权衡问题。

当然，写入的性能很难超过简单的追加文件，因为只是最简单的写入操作。任何索引都会减慢写入速度，因为每次写入数据都需更新索引。

## Hash 索引

即在内存中保存一个哈希映射，hash map的key为上边数据库的键值，值为实际内容的字节偏移量。

当数据库中存储新的值时，除了将新的键值对追加到文件外，还需要更新内存中的hash映射（也适用于插入、更新现有键值对）；当查找一个值时，可以使用哈希映射来查找数据文件中的偏移量，寻找（seek）该位置并读取该值。

现实中，Bitcask（默认 Riak 引擎） 就是这么做的。Ritcask提供高性能读写的必要条件是所有的 **key** 都放置在可用RAM中，因为哈希映射完全在内存中。 **value** 可以使用比内存大的空间，因为它们可以通过磁盘seek加载。如果部分数据已经存在在文件系统缓存中，读取则不需要磁盘 **I/O** 了。

像 Bitcask 这样的存储引擎适合每个键经常更新的情况。

## Sorted String Table 对比 Hash 的优点

1、合并段简单高效。

2、查找特定的键，不需要内存中所以的索引。

3、因为读请求无论如何都有扫描一部分键值对，因此在写入磁盘前，可以将记录分组、压缩，并在内存中维护一个压缩块开始的key和偏移量的索引。既可以节约磁盘空间，也可以减少IO带宽的使用。

### 构建和维护 SSTables

b树，在磁盘上维护有序结构。

红黑树、AVL树，在内存中维护有序结构。

1. 写入时，将其添加到**内存**中的平衡数据结构（例如，红黑树）。这种内存中的树有时叫 **内存表（memtable）**。
2. 当内存表大于某个阈值（通常几兆）时，将其作为 SSTable 文件写入磁盘。树已经维护了排序的键值对，所以写入可以高效完成。新的 SSTable 文件成为数据库最新的部分。当 SSTable 正在被写入磁盘时，第一步的写入操作可以继续到一个新的内存表实例中。
3. 为了提供读取请求，首先尝试在内存表中找到关键字，然后在最近的磁盘段中，然后在下一个较旧的段中找关键字。
4. 不时，会在后台运行合并和压缩进程以组合文件段、丢弃覆盖和删除值。

此方案问题：如果数据库崩溃，最近写入的（在内存表中，还未写入磁盘）数据将丢失。

解决问题：在磁盘上保存一个单独的日志，每个写入操作都被立刻追加到该文件中。此操作日志的唯一目的是为了崩溃后恢复内存表。所以每当内存表中的数据被当作SSTable文件写入磁盘时，相应的日志文件都会被丢弃。

### 用 SSTables 制作 LSM Tree

该算法本质就是 LevelDB （google 开发） 和 RocksDB（facebook 开发）所使用的算法，它们都是被设计成可以嵌入到其他应用程序中的键值存储引擎库。另外LevelDB可以作为Bitcask的替代选项，在Riak中使用。类似的存储引擎也被用于 Cassandra 和 HBase 中。

### 性能优化

1. 查找数据库中不存在的键，LSM-tree 算法很慢：必须先检查内存表，然后检查磁盘文件端一直到最久的数据（可能需要读取每一个），才能确定键不存在。通常使用存储引擎通常使用额外的**Bloom过滤器**解决这种问题。

2. 选择SSTables压缩和合并的顺序和时机会有不同的策略。最常见的策略有 **大小分层**（*Size-tiered Compaction*）和 **平坦压缩** （*Leveled compaction*）。LevelDB 和 RocksDB 使用了 平坦压缩 ，HBase 使用了 大小分层， Cassandra 同时都支持。

   1. Size-Tiered Compaction 策略：memtable 逐步刷入到磁盘 sst，刚开始 sst 都是小文件，随着小文件越来越多，当数据量达到一定阈值时，STCS 策略会将这些小文件 compaction 成一个中等大小的新文件。同样的道理，当中等文件数量达到一定阈值，这些文件将被 compaction 成大文件，这种方式不断递归，会持续生成越来越大的文件。数据合并不是即时的（达到阈值合并），相同数据的文件，可能存在于不同大小等级的多个文件中，故存在 **空间放大**（实际数据量只有1G，可能存储过程中占用远超1G）。对于覆写频繁的场景并不适用。

   2. Leveled compaction 策略：

      1. sst 的大小可控，默认每个 sst 的大小一致（Size-Tiered Compaction 策略最终会产生超大文件）
      2. LCS 在合并时，会保证除 Level 0（L0）之外的其他 Level 有序且无覆盖。
      3. 除 L0 外，每层文件的总大小呈指数增长，假如 L1 最多 10 个，则 L2 为 100 个，L3 为 1000 个...

      关键范围被拆分成更小的SSTables，而较旧的数据被移动到单独的“水平”，这使得压缩能够更加递增地进行，并且使用更少的磁盘空间。

      该策略会造成 **写放大** 。

LSM树的基本思想 —— 保存一系列在后台合并的SSTables —— 简单而有效。即使数据集比可用内存大得多，它仍能继续正常工作。由于数据按排序顺序存
储，因此可以高效地执行范围查询（扫描所有高于某些最小值和最高值的所有键），并且因为磁盘写入是连续的，所以LSM树可以支持非常高的写入吞吐量。

### 参考链接

[Compaction 策略 - Size-Tiered](https://zhuanlan.zhihu.com/p/462574732)

[Compaction 策略 -- Leveled](https://zhuanlan.zhihu.com/p/462850000)

